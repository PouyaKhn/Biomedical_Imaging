# -*- coding: utf-8 -*-
"""task3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ncgd23fUYkIXEwEFPs355AZESzCZsu2G

### Problem:  building an algorithm to detect a visual signal for pneumonia in medical images. Specifically, this algorithm needs to automatically locate lung opacities on chest radiographs.

**We have to predict bounding boxes on CXR images that are locating suspicious areas. It is aboslutely an object detection task.**

We have 3 classes in this dataset but only the **Lung Opacity** class is important for us because the other two do not have boxes.

Our chosen model for this task is a state-of-the-art architecture called Detectron2.
"""

## install pydicom
!pip install pydicom

!pip install pyyaml==5.1

import torch
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
# Install detectron2 that matches the above pytorch version

!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

import os
import sys
import pandas as pd
import pickle
import sys
from collections import defaultdict
import math
import random
import skimage.io
import skimage.transform
from skimage.transform import SimilarityTransform, AffineTransform
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import gridspec
from tqdm.auto import tqdm, trange
import pydicom
import cv2
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from io import BytesIO  
from io import StringIO
import scipy.misc
from torch import nn, optim

"""## 1. Load Data



"""

from google.colab import files
## Upload cookies.txt
files.upload()

!wget -x --load-cookies ./cookies.txt "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/10338/862042/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1639201669&Signature=ok1wUW5obqiLkXSKckFR0V4ANeqnGMqZvfG0HZ4nyE1Yipg%2FQSzax%2BDBEOtzIDYANQL5AVChwz5iml1ofOLi7PcMV4AuF1%2FcCmvc7Yizgdh3FVmIQQNdKW784tTotVi26DAl5CUtxXzdbpHGEvs2IeLujCantnK2BiBWCchXWTf1oATkjn7Sn2Lfu5bx3ZxVe7dZ%2BRrYbbSxE2PcbRdIofjY%2Fa9OcQfJ2CThrZJj4pjbUHhO%2BNQF2ZNRJKloHVbmHlvM9LvFM5dUczTnBZEEymOwKyojfgTRNH0qwWNKdeIAbUBp5IXlxp6bNXUokfyvFioq7DHJ8EaG%2BaNVup128w%3D%3D&response-content-disposition=attachment%3B+filename%3Drsna-pneumonia-detection-challenge.zip" -O data.zip

## Unzip data
!unzip data.zip

"""## Save data"""

for dcm in os.listdir('stage_2_train_images'):
    #### covert dicom to png
    ds = pydicom.read_file('./stage_2_train_images/'+dcm)
    img = ds.pixel_array.astype(float)
    #### resize
    img_scaled = skimage.transform.resize(img, (512, 512), anti_aliasing=True)
    img_scaled = np.uint8(img_scaled)
    #### save image
    cv2.imwrite('./stage_2_train_images/' + dcm.replace('.dcm','.png'), img_scaled)
    os.system('rm stage_2_train_images/'+ dcm)

"""## Preparation



"""

df1 = pd.read_csv('./stage_2_train_labels.csv')

df2 = pd.read_csv('./stage_2_detailed_class_info.csv')

print(f'Ground Truth boxes size: {len(df1)}, Patient class size: {len(df2)}')

df1.head()

df2.head()

samples = None
## merge 
df = pd.merge(df1, df2, on='patientId')

## keep Lung Opacity
samples = pd.DataFrame(df[df['class']=='Lung Opacity'])
samples.drop_duplicates(inplace = True,  ignore_index=True)
print(len(samples))

"""### Split patients to validation and train"""

train_samples = None
val_samples = None
## Split
msk = np.random.rand(len(samples)) < 0.8
train_df = samples[msk]
val_df = samples[~msk]

#Finding indexes of validation rows which has the same 'patientId' as train data's.
intersect = pd.merge(train_df, val_df, on = 'patientId')
inter_idx = np.array([val_df.index[val_df['patientId']==value].tolist() for value in intersect['patientId'].values], dtype=object)

idx_flat = []
for l in inter_idx:
    idx_flat.extend(l)

#Removing duplicates
idx_flat = list(dict.fromkeys(idx_flat))

# Adding back to train datasets
train_samples = train_df.append(val_df.loc[idx_flat], ignore_index=True)
val_samples = val_df.drop(idx_flat)
val_samples.reset_index(drop=True, inplace=True)
print('Train and Validation size (Before) :({},{})'.format(len(train_df), len(val_df)))
print('Train and Validation size (After) :({},{})'.format(len(train_samples), len(val_samples)))

# Group by Patient ID
train_samples = train_samples.groupby(['patientId'], dropna=True)
val_samples = val_samples.groupby(['patientId'], dropna=True)

train_samples.head()

"""## Dataset
When we rotate or scale an image, we have to do all transformation on bounding boxes too. 



"""

from detectron2.structures import BoxMode

whole = {'train': train_samples, 'val': val_samples}

def get_CXR_dicts(string, img_size = 512):

    if string == 'train':
        is_training = True
    else:
        is_training = False

    samples = whole[string]
    dataset_dicts = []
    for name, group in samples:
        record = {}
        ## filename is the address of patient image (I already saved the preprocessed images on my google drive)
        filename = str(f'./stage_2_train_images/{name}.png')
        record["file_name"] = filename
        ## fill None with appropriate values
        record["image_id"] = name
        record["height"] = img_size # Resized shape of image
        record["width"] = img_size
        
        objs = []
        for _, row in group.iterrows():
            ###  each group represents a patient, and the rows of the specific group shows bounding boxes for that patient
            resize_ratio = 0.5 # 512/1024
            x = int(row['x'])
            y = int(row['y'])
            width = int(row['width'])
            height = int(row['height'])

            x = int(round(x*resize_ratio))
            y = int(round(y*resize_ratio))
            w = int(round(width*resize_ratio))
            h = int(round(height*resize_ratio))          
            bbox_resized = [x, y, w, h]
                
            obj = {
                "bbox": bbox_resized,
                "bbox_mode": BoxMode.XYWH_ABS,
                "category_id": 0,
            }
            ### objs is list of bounding boxes of a particular patient
            objs.append(obj)

        record["annotations"] = objs

        dataset_dicts.append(record)

    return dataset_dicts

### fill the attributes
for d in ["train", "val"]:
    DatasetCatalog.register("CXR_" + d, lambda d=d: get_CXR_dicts(d))
    MetadataCatalog.get("CXR_" + d).set(thing_classes=["opacity"])

CXR_metadata = MetadataCatalog.get("CXR_train")

"""### Visualizing three train patients with their annotations"""

dataset_dicts = get_CXR_dicts("train")

for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=CXR_metadata, scale=0.5)
    out = visualizer.draw_dataset_dict(d)
    cv2_imshow(out.get_image()[:, :, ::-1])

"""we can change settings like LR or iterations here to get better performance.

We train detectron2 with two different baselines:

1.   RetinaNet
2.   FasterRCNN

and then we compare the results.



"""

from detectron2.engine import DefaultTrainer

cfg = get_cfg()

cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml"))


cfg.DATASETS.TRAIN = ("CXR_train",)
cfg.DATASETS.TEST = ()
cfg.DATALOADER.NUM_WORKERS = 2

cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml")  # Let training initialize from model zoo

cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025
cfg.SOLVER.MAX_ITER = 500
cfg.SOLVER.STEPS = []
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)



cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1

cfg.OUTPUT_DIR = './output_RCNN'
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()

cfg1 = get_cfg()

cfg1.merge_from_file(model_zoo.get_config_file("COCO-Detection/retinanet_R_50_FPN_1x.yaml"))


cfg1.DATASETS.TRAIN = ("CXR_train",)
cfg1.DATASETS.TEST = ()
cfg1.DATALOADER.NUM_WORKERS = 2


cfg1.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/retinanet_R_50_FPN_1x.yaml")  

cfg1.SOLVER.IMS_PER_BATCH = 2
cfg1.SOLVER.BASE_LR = 0.00025
cfg1.SOLVER.MAX_ITER = 500
cfg1.SOLVER.STEPS = []        
cfg1.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   



cfg1.MODEL.ROI_HEADS.NUM_CLASSES = 1

cfg1.OUTPUT_DIR = './output_Retina'
os.makedirs(cfg1.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg1) 
trainer.resume_or_load(resume=False)
trainer.train()

#RCNN
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")  
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7
predict_RCNN = DefaultPredictor(cfg)

#Retina
cfg1.MODEL.WEIGHTS = os.path.join(cfg1.OUTPUT_DIR, "model_final.pth")
cfg1.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   
predictor_Retina = DefaultPredictor(cfg1)

"""#### Results of Faster-RCNN:"""

######
import pandas as pd
metrics_df = pd.read_json("./output_RCNN/metrics.json", orient="records", lines=True)
mdf = metrics_df.sort_values("iteration")
mdf.head(10).T

"""#### Results of Retinanet:"""

########
import pandas as pd
metrics_df = pd.read_json("./output_Retina/metrics.json", orient="records", lines=True)
mdf = metrics_df.sort_values("iteration")
mdf.head(10).T

"""#### Visualize some validation images with their predicted bounding boxes"""

val_metadata = MetadataCatalog.get("CXR_val")
val_dataset_dicts = get_CXR_dicts("val")

##### visualize 
# RCNN
for d1 in random.sample(val_dataset_dicts, 3):    
    im1 = cv2.imread(d1["file_name"])
    outputs1 = predict_RCNN(im1)
    v = Visualizer(im1[:, :, ::-1], metadata = val_metadata, scale = 0.5)
    for box in outputs1["instances"].pred_boxes.to('cpu'):
        v.draw_box(box)
        v.draw_text(str(box[:2].numpy()), tuple(box[:2].numpy()))
    v = v.get_output()
    img = v.get_image()[:, :, ::-1]
    cv2_imshow(img)

for d2 in random.sample(val_dataset_dicts, 3):    
    im2 = cv2.imread(d2["file_name"])
    outputs2 = predictor_Retina(im2)
    v1 = Visualizer(im2[:, :, ::-1], metadata = val_metadata, scale = 0.5)
    for box in outputs2["instances"].pred_boxes.to('cpu'):
        v1.draw_box(box)
        #v1.draw_text(str(box[:2].numpy()), tuple(box[:2].numpy()))
    v1 = v1.get_output()
    img1 = v1.get_image()[:, :, ::-1]
    cv2_imshow(img1)

"""### Inference"""

#RCNN
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader
evaluator = COCOEvaluator("CXR_val", output_dir="./output_RCNN")
val_loader = build_detection_test_loader(cfg, "CXR_val")
print(inference_on_dataset(predict_RCNN.model, val_loader, evaluator))

#Retina
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader
evaluator = COCOEvaluator("CXR_val", output_dir="./output_Retina")
val_loader = build_detection_test_loader(cfg1, "CXR_val")
print(inference_on_dataset(predictor_Retina.model, val_loader, evaluator))